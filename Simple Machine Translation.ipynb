{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Neural Machine Translation\n",
    "###  By Chandra S Narain Kappera\n",
    "\n",
    "### Dataset:\n",
    "Tab seprated German and English Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = to_pairs(load_doc('deu.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Get away!', 'Verpiss dich!'],\n",
       " ['Get away!', 'Hau ab!'],\n",
       " ['Get away!', 'Verschwinde!'],\n",
       " ['Get away!', 'Verdufte!'],\n",
       " ['Get away!', 'Mach dich fort!'],\n",
       " ['Get away!', 'Zieh Leine!'],\n",
       " ['Get away!', 'Mach dich vom Acker!'],\n",
       " ['Get away!', 'Verzieh dich!'],\n",
       " ['Get away!', 'VerkrÃ¼mele dich!'],\n",
       " ['Get away!', 'Troll dich!']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[200:210]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "Remove all punctuation characters.  \n",
    "Normalize all Unicode characters to ASCII (e.g. Latin characters).  \n",
    "Normalize the case to lowercase.  \n",
    "Remove any remaining tokens that are not alphabetic.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "from pickle import dump\n",
    "\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: clean_data.pkl\n"
     ]
    }
   ],
   "source": [
    "clean_pairs = clean_pairs(data)\n",
    "save_clean_data(clean_pairs,'clean_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hi] => [hallo]\n",
      "[hi] => [gru gott]\n",
      "[run] => [lauf]\n",
      "[wow] => [potzdonner]\n",
      "[wow] => [donnerwetter]\n",
      "[fire] => [feuer]\n",
      "[help] => [hilfe]\n",
      "[help] => [zu hulf]\n",
      "[stop] => [stopp]\n",
      "[wait] => [warte]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    " \n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    " \n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('clean_data.pkl')\n",
    " \n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "# save\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    " \n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map text to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2516\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3871\n",
      "German Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the NMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 256)           990976    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 2516)           646612    \n",
      "=================================================================\n",
      "Total params: 2,688,212\n",
      "Trainable params: 2,688,212\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    " \n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# summarize defined model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.61593, saving model to model.h5\n",
      " - 48s - loss: 4.3741 - val_loss: 3.6159\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.61593 to 3.47968, saving model to model.h5\n",
      " - 51s - loss: 3.4355 - val_loss: 3.4797\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.47968 to 3.40885, saving model to model.h5\n",
      " - 50s - loss: 3.2929 - val_loss: 3.4088\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.40885 to 3.30110, saving model to model.h5\n",
      " - 49s - loss: 3.1791 - val_loss: 3.3011\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.30110 to 3.16015, saving model to model.h5\n",
      " - 49s - loss: 2.9956 - val_loss: 3.1602\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.16015 to 3.05548, saving model to model.h5\n",
      " - 50s - loss: 2.8204 - val_loss: 3.0555\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.05548 to 2.96088, saving model to model.h5\n",
      " - 50s - loss: 2.6625 - val_loss: 2.9609\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.96088 to 2.83820, saving model to model.h5\n",
      " - 50s - loss: 2.5032 - val_loss: 2.8382\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.83820 to 2.73212, saving model to model.h5\n",
      " - 51s - loss: 2.3520 - val_loss: 2.7321\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.73212 to 2.67387, saving model to model.h5\n",
      " - 51s - loss: 2.2039 - val_loss: 2.6739\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.67387 to 2.57743, saving model to model.h5\n",
      " - 52s - loss: 2.0659 - val_loss: 2.5774\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.57743 to 2.53371, saving model to model.h5\n",
      " - 50s - loss: 1.9375 - val_loss: 2.5337\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.53371 to 2.45939, saving model to model.h5\n",
      " - 51s - loss: 1.8149 - val_loss: 2.4594\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.45939 to 2.40388, saving model to model.h5\n",
      " - 50s - loss: 1.6985 - val_loss: 2.4039\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.40388 to 2.35903, saving model to model.h5\n",
      " - 50s - loss: 1.5876 - val_loss: 2.3590\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.35903 to 2.31715, saving model to model.h5\n",
      " - 50s - loss: 1.4850 - val_loss: 2.3171\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.31715 to 2.26873, saving model to model.h5\n",
      " - 49s - loss: 1.3824 - val_loss: 2.2687\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 00018: val_loss improved from 2.26873 to 2.24113, saving model to model.h5\n",
      " - 54s - loss: 1.2902 - val_loss: 2.2411\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 00019: val_loss improved from 2.24113 to 2.22373, saving model to model.h5\n",
      " - 59s - loss: 1.1971 - val_loss: 2.2237\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 00020: val_loss improved from 2.22373 to 2.20335, saving model to model.h5\n",
      " - 61s - loss: 1.1084 - val_loss: 2.2033\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 00021: val_loss improved from 2.20335 to 2.17353, saving model to model.h5\n",
      " - 62s - loss: 1.0220 - val_loss: 2.1735\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 00022: val_loss improved from 2.17353 to 2.16423, saving model to model.h5\n",
      " - 59s - loss: 0.9415 - val_loss: 2.1642\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 00023: val_loss improved from 2.16423 to 2.13687, saving model to model.h5\n",
      " - 58s - loss: 0.8665 - val_loss: 2.1369\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 00024: val_loss improved from 2.13687 to 2.13468, saving model to model.h5\n",
      " - 58s - loss: 0.7936 - val_loss: 2.1347\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 00025: val_loss improved from 2.13468 to 2.12459, saving model to model.h5\n",
      " - 58s - loss: 0.7259 - val_loss: 2.1246\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.12459 to 2.11482, saving model to model.h5\n",
      " - 59s - loss: 0.6636 - val_loss: 2.1148\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      " - 62s - loss: 0.6056 - val_loss: 2.1194\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      " - 66s - loss: 0.5515 - val_loss: 2.1336\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 00029: val_loss improved from 2.11482 to 2.10436, saving model to model.h5\n",
      " - 60s - loss: 0.5025 - val_loss: 2.1044\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      " - 57s - loss: 0.4586 - val_loss: 2.1102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc031d6f28>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append(raw_target.split())\n",
    "        predicted.append(translation.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src=[alle mann von bord], target=[abandon ship], predicted=[face aboard]\n",
      "src=[wie kann ich mich nutzlich machen], target=[how can i help], predicted=[why did i ask]\n",
      "src=[klapp dein buch zu], target=[close your book], predicted=[the your hat]\n",
      "src=[ich will das], target=[i want that], predicted=[i want this]\n",
      "src=[ihr werdet euch verlaufen], target=[youll get lost], predicted=[youll look you]\n",
      "src=[sie sind ein feigling], target=[youre a coward], predicted=[youre a coward]\n",
      "src=[zeig sie ihr], target=[show it to her], predicted=[show it to her]\n",
      "src=[ich brauche nur tom], target=[i only need tom], predicted=[i need to tom]\n",
      "src=[ich erinnere mich], target=[i remember], predicted=[i surrender]\n",
      "src=[ich mochte das gleiche], target=[i want the same], predicted=[i want want too]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narai\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.067749\n",
      "BLEU-2: 0.253323\n",
      "BLEU-3: 0.429322\n",
      "BLEU-4: 0.489847\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning**  Different data cleaning operations could be performed on the data, such as not removing punctuation or normalizing case, or perhaps removing duplicate English phrases.   \n",
    "**Vocabulary** The vocabulary could be refined, perhaps removing words used less than 5 or 10 times in the dataset and replaced with âunkâ.  \n",
    "**More Data** - The dataset used to fit the model could be expanded to 50,000, 100,000 phrases, or more.  \n",
    "**Layers** The encoder and/or the decoder models could be expanded with additional layers and trained for more epochs, providing more representational capacity for the model.  \n",
    "**Units** The number of memory units in the encoder and decoder could be increased, providing more representational capacity for the model.  \n",
    "**Regularization** The model could use regularization, such as weight or activation regularization, or the use of dropout on the LSTM layers.  \n",
    "**Pre-Trained Word Vectors** Pre-trained word vectors could be used in the model.  \n",
    "Recursive Model. A recursive formulation of the model could be used where the next word in the output sequence could be conditional on the input sequence and the output sequence generated so far.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspired by: MachineLearningMastery.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
